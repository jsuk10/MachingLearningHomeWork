[→ Open in Slid](https://slid.cc/vdocs/7c4fa46ca740474484e35d8389d45c79)


---




## 릿지 회귀




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/e33ec59c-fb32-4746-ad48-a00c7dcc51f6.png "5강 릿지, 라소 image")


 - 선형모델에 추가로 가중치를 최소화 하는 방법
 - 최소 적합법 예측함수를 여전히 이용함.
 - w : 훈련 데이터 예측
 - 설명 변수(특징) 수가 증가할 경우
 - 설명 변수들 사이의 다중 공선성이 가능
 - 독립 변수들 간에 상관관계가 높아진다.
 - 모델이 지나치게 복잡해진다.
 - 회귀계수의 축소
 - 계수(w)의 영향력을 축소 하여 모델을 간결화 할 수 있다.

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/7e181c2b-21c8-4169-bb1c-df3a3b7c28d6.png "5강 릿지, 라소 image")


Bj^2에 ㅅ(람다)를 붙여서 패널티를 조절한다.


mse(앞부분) + 패널티를 최소로 하는 w, b를 찾아야 한다.




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/957627ca-0174-4c12-b3e9-1343dccf1feb.png "5강 릿지, 라소 image")




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/20a20981-2be2-4585-a456-a45c0d3e87d8.png "5강 릿지, 라소 image")


 - 가중치의 절대값을 가능한 작게
 - w들이 모두 0에 가깝게
 - 모든 특성이 출력에 주는 영향을 최소로 만들기 위해
 - 기울기를 작게 : 규제/제약(Regularization)
 - 과대 적합이 되지 않도록 모델을 강제로 제한한다.
 - L2규제 라고 말함

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/868bdf1d-8243-41cf-88f4-207efe50f5ce.png "5강 릿지, 라소 image")


 - 패널티를 조절하는 모수(람다 ㅅ) tuning parameter
 - 과적합을 방지하기 위해 잔차를 계한하는 cost함수에 패널티를 부여함
 - 순수한 오차값에 과하게 반응하지 않게 해서 과적합을 방지한다.
 - 람다가 클수록 coefficient를 축소 시킨다.
 - 회귀추정치와 관련된 영향을 조절




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/1f7d7323-3251-4d54-bf28-c79e0acac59c.png "5강 릿지, 라소 image")


람다가 0이 되면 패널티가 효과가 없다.


 - 복잡도 증가, 과대적합 가능성이 커짐


람다가 커질수록 앞의 값이 영향이 거의 없어진다


 - 복잡도 감소, 과소적합 가능성이 커짐





명시적 람다의 지정이 필요


default = 1.0


특징들이 비슷한 중요도(릿지)

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/32da1015-bf21-4d35-ae68-a73fe5f17158.png "5강 릿지, 라소 image")


릿지가 테스트 점수에서 다른 두 모델보다 높은 점수를 얻는 것을 볼 수 있음.




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/68b3239e-f64d-49fe-8e6d-0953d2d59b8e.png "5강 릿지, 라소 image")




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/c72a3533-4832-4689-b837-bdac658285ad.png "5강 릿지, 라소 image")


릿지와 리니어리그레션 볼경우 차이점을 볼 수 있다.


훈련세트가 커지면 둘이 서로 비슷하게 된다.




선형 모델일 경우- 훈련집합에 대해 높은 성능


 - 과대적합
 - 데이터 증가시 과대적합 감소
 - 적은수의 훈련데이터에 대해 R^2&lt;0





릿지 모델 경우, a=1


 - 테스트 집합에 대해 높은 성능
 - 데이터 증가시 규제효과 감소
 - <b>복잡한 모델 가능</b>




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/a82f8e91-7abd-438d-9d0d-31842030e900.png "5강 릿지, 라소 image")


라쏘 회귀


 - 가중치(w)의 영향을 최소화 시킨다.
 - 릿지와 동일하게 선형 모델임.
 - 계수의 편차를 감소 시킨다
 - 릿지와 다른점
 - 패널티가 L2이기 때문에 릿지는 0이 될수 없는데 릿지는 L1이기 떄문에 0이 될 수 있음
 - -&gt; 특성을 선별해서 해석력을 보인다.


a를 크게 하면 패널티 효과가 크게 된다.


과소 적합이 된다(데이터 특징 반영 잘 안됨)





a를 작게 하면 MSE 비중이 증가된다.


과되 적합이 된다(복잡도 증가, 훈련 데이터의 특징이 많이 반영됨)





일부 특징들 중에 중요한 것들이 몇개 보이고, 몇개는 제거되지 않을경우


<b>-&gt; 일부의 특징만 관찰하고 싶을때 사용</b>




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/e8fde2cb-0b2d-4994-bd62-538df37474c1.png "5강 릿지, 라소 image")


defult 값인 alpha = 1임.




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/b31bddad-66b7-4225-bd67-d93a2206d0cd.png "5강 릿지, 라소 image")





alpha 가1일 경우 낮은 성능, 적은 특성 사용됨


alpha 가 0.01일때 패널티 효과 감소하고, 복잡도가 증가한다


alpha 를 아주 적게 하면 선형 모델과 유사해짐





특성의 개수를 채로 걸러내는 느낌인가..???

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/b874e005-b03a-4ab5-b154-5d3f9961a929.png "5강 릿지, 라소 image")


라소와 엣지들 비교




## 릿지 vs 라소

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/d323d56e-b45b-4657-9adf-333f4c6144fc.png "5강 릿지, 라소 image")




## Norm

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/6f6669a6-82c2-44b6-a298-822f9d89db70.png "5강 릿지, 라소 image")


벡터의 길이/크기를 측정하는 방법임


<b>개 중요함.!!</b>


p == 1 L1


 - 절대값들의 합이 됨
 - 맨하탄


p == 2 L2


 - 절대값의 제곱들의 합이됨
 - 유클리디안




## 로지스틱 회귀 분석

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/d67cda83-f8d2-4e5d-a045-4352ac1a8167.png "5강 릿지, 라소 image")


<b>분류용 </b>선형 모델임


2가지의 값만 가지는 모델(a/b로만 선택 가능)


y = ax+b


a = w라고 할때


y = w[0] x X[0] + b


w를 찾는 것이 목적(a)


출력 : 가중치 합 + 패널티 를 예측값(확률값)으로 출력 가능


 - 독립변수의 결합을 통해 사건의 발생 가능성을 예측


p = ax + b 를 임계치와 비교


 - 예측값 &lt; 임계치 : -1
 - 예측값 &gt; 임계치 : 1





Q: ax + b 는 [ - inf(무한) ~ inf] 이고 우리는 [0~1]사이의 값을 원한다.




## 오즈비 (Odds Ratio)

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/f45048f9-f13f-4c42-80ab-b14f20a0c19d.png "5강 릿지, 라소 image")


특정 이벤트의 확률

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/5770eb7e-fdf1-4f66-bea5-2a1f8f62d7ae.png "5강 릿지, 라소 image")


0~inf까지 됨


P : Probalbillty of Positive event


log에 넣게 되면 선형으로 나옴


로그 오즈비를 통해 -inf~ inf까지 선형화가 가능하다.

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/cc4cec50-e7c5-41bc-8e55-8b23cf9f9ec1.png "5강 릿지, 라소 image")




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/bc7f4b70-5157-4fd0-a65a-734deda0771c.png "5강 릿지, 라소 image")




## 로지스틱 정리

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/3fcd7746-962a-49fc-84a4-03c2fe639ad8.png "5강 릿지, 라소 image")


 - 확률을 생성하고 확률을 임계치로 자른다.




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/190aa628-8465-4afb-ac26-a74817888786.png "5강 릿지, 라소 image")


 - 종속변수와 독립변수간의 관계를 함수로 나타내서 예측 모델로 사용
 - 입력 데이터가 주어질 경우 해당 데이터 결과가 특정 클래스로 분류됨
 - <b>분류 기법( Classification)기법</b>
 - Logistic Function
 - 선형이 아닌 s자형 커브로 나오게됨.




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/1b57d927-c82d-437a-9cac-ecbd97c647d5.png "5강 릿지, 라소 image")


최소 제곱법 처럼 W와 B를 구하는 것이 핵심이다.

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/0574f08f-c773-4ccb-a093-bcc35c220a61.png "5강 릿지, 라소 image")


앞에서 있던 패널티도 같게 쓰임




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/38cd6111-6c1d-42ac-a657-4b52b6137ffc.png "5강 릿지, 라소 image")


* * * *을 보면 0/1로 구분 되어 있다.


초록은 Virginica인 친구들의 가능성


파랑은 Virginica가 아닌 친구들의 가능성


만나는 점은 결정 경계로 형성된다.

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/c0a2be09-54f1-4ac9-a16f-e562f44c41e8.png "5강 릿지, 라소 image")


선형과 Logistic으로 분류된 경우


결과에서 하나씩 잘못 분류된 경우가 보인다.




![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/f86e26cd-05b8-4915-8a7c-d069197df944.png "5강 릿지, 라소 image")


cacer데이터 분류시


 1. c= 100을 주었을때<br>역수로 취해주기 때문에 1/100 이 되어 패널티가 작아지고 규제가 완화된다
 2. c = 0.01로 주었을때<br>역수로 취해지기 떄문에 100이 되며 규제강화 패널티가 커진다, 계수가 0에 가까워진다

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/3b955f79-92c3-4168-8aeb-4bc5a59c4637.png "5강 릿지, 라소 image")


c = defult값이 1.0이다.

![](https://slid-capture.s3.ap-northeast-2.amazonaws.com/public/capture_images/7c4fa46ca740474484e35d8389d45c79/0378a863-75c2-43c5-9582-185f08d17676.png "5강 릿지, 라소 image")



